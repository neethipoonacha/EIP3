{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neethipoonacha/EIP3/blob/master/Copy_of_DN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOsYqWHvQBrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import time, math\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "#from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.callbacks import Callback\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcVY1UhnQPDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.enable_eager_execution()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LfLeMAyQU7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 512 #@param {type:\"integer\"}\n",
        "MOMENTUM = 0.9 #@param {type:\"number\"}\n",
        "LEARNING_RATE = 0.4 #@param {type:\"number\"}\n",
        "WEIGHT_DECAY = 5e-4 #@param {type:\"number\"}\n",
        "EPOCHS = 24 #@param {type:\"integer\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwS9BRv_QWR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
        "  fan = np.prod(shape[:-1])\n",
        "  bound = 1 / math.sqrt(fan)\n",
        "  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQdPrpJ8QXvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvBN(tf.keras.Model):\n",
        "  def __init__(self, c_out):\n",
        "    super().__init__()\n",
        "    self.conv = tf.keras.layers.Conv2D(filters=c_out, kernel_size=3, padding=\"SAME\", kernel_initializer=init_pytorch, use_bias=False)\n",
        "    self.bn = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.nn.relu(self.bn(self.conv(inputs)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBuqNlb7QZc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResBlk(tf.keras.Model):\n",
        "  def __init__(self, c_out, pool, res = False):\n",
        "    super().__init__()\n",
        "    self.conv_bn = ConvBN(c_out)\n",
        "    self.pool = pool\n",
        "    self.res = res\n",
        "    if self.res:\n",
        "      self.res1 = ConvBN(c_out)\n",
        "      self.res2 = ConvBN(c_out)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    h = self.pool(self.conv_bn(inputs))\n",
        "    if self.res:\n",
        "      h = h + self.res2(self.res1(h))\n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB5ENbbjQa7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DavidNet(tf.keras.Model):\n",
        "  def __init__(self, c=64, weight=0.125):\n",
        "    super().__init__()\n",
        "    pool = tf.keras.layers.MaxPooling2D()\n",
        "    self.init_conv_bn = ConvBN(c)\n",
        "    self.blk1 = ResBlk(c*2, pool, res = True)\n",
        "    self.blk2 = ResBlk(c*4, pool)\n",
        "    self.blk3 = ResBlk(c*8, pool, res = True)\n",
        "    self.pool = tf.keras.layers.GlobalMaxPool2D()\n",
        "    self.linear = tf.keras.layers.Dense(10, kernel_initializer=init_pytorch, use_bias=False)\n",
        "    self.weight = weight\n",
        "\n",
        "  def call(self, inputs):\n",
        "    h = self.pool(self.blk3(self.blk2(self.blk1(self.init_conv_bn(x)))))\n",
        "    h = self.linear(h) * self.weight\n",
        "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h, labels=y)\n",
        "    loss = tf.reduce_sum(ce)\n",
        "    correct = tf.reduce_sum(tf.cast(tf.math.equal(tf.argmax(h, axis = 1), y), tf.float32))\n",
        "    return loss, correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQC9BguoQcfp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b3faf912-7e61-4bed-8b84-f999085156ec"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "len_train, len_test = len(x_train), len(x_test)\n",
        "y_train = y_train.astype('int64').reshape(len_train)\n",
        "y_test = y_test.astype('int64').reshape(len_test)\n",
        "\n",
        "train_mean = np.mean(x_train, axis=(0,1,2))\n",
        "train_std = np.std(x_train, axis=(0,1,2))\n",
        "\n",
        "normalize = lambda x: ((x - train_mean) / train_std).astype('float32') # todo: check here\n",
        "pad4 = lambda x: np.pad(x, [(0, 0), (4, 4), (4, 4), (0, 0)], mode='reflect')\n",
        "\n",
        "x_train = normalize(pad4(x_train))\n",
        "x_test = normalize(x_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 9s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbOrV1Mv35F-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LR_Finder(Callback):\n",
        "    \n",
        "    def __init__(self, start_lr=1e-5, end_lr=10, step_size=None, beta=.98):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.start_lr = start_lr\n",
        "        self.end_lr = end_lr\n",
        "        self.step_size = step_size\n",
        "        self.beta = beta\n",
        "        self.lr_mult = (end_lr/start_lr)**(1/step_size)\n",
        "        \n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.best_loss = 1e9\n",
        "        self.avg_loss = 0\n",
        "        self.losses, self.smoothed_losses, self.lrs, self.iterations = [], [], [], []\n",
        "        self.iteration = 0\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.lr, self.start_lr)\n",
        "        \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        loss = logs.get('loss')\n",
        "        self.iteration += 1\n",
        "        \n",
        "        self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * loss\n",
        "        smoothed_loss = self.avg_loss / (1 - self.beta**self.iteration)\n",
        "        \n",
        "        # Check if the loss is not exploding\n",
        "        if self.iteration>1 and smoothed_loss > self.best_loss * 4:\n",
        "            self.model.stop_training = True\n",
        "            return\n",
        "\n",
        "        if smoothed_loss < self.best_loss or self.iteration==1:\n",
        "            self.best_loss = smoothed_loss\n",
        "        \n",
        "        lr = self.start_lr * (self.lr_mult**self.iteration)\n",
        "        \n",
        "        self.losses.append(loss)\n",
        "        self.smoothed_losses.append(smoothed_loss)\n",
        "        self.lrs.append(lr)\n",
        "        self.iterations.append(self.iteration)\n",
        "        \n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, lr)  \n",
        "        \n",
        "    def plot_lr(self):\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.ylabel('Learning rate')\n",
        "        plt.plot(self.iterations, self.lrs)\n",
        "        \n",
        "    def plot(self, n_skip=10):\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Learning rate (log scale)')\n",
        "        plt.plot(self.lrs[n_skip:-5], self.losses[n_skip:-5])\n",
        "        plt.xscale('log')\n",
        "        \n",
        "    def plot_smoothed_loss(self, n_skip=10):\n",
        "        plt.ylabel('Smoothed Losses')\n",
        "        plt.xlabel('Learning rate (log scale)')\n",
        "        plt.plot(self.lrs[n_skip:-5], self.smoothed_losses[n_skip:-5])\n",
        "        plt.xscale('log')\n",
        "        \n",
        "    def plot_loss(self):\n",
        "        plt.ylabel('Losses')\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.plot(self.iterations[10:], self.losses[10:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-rBodFhQeNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = DavidNet()\n",
        "batches_per_epoch = len_train//BATCH_SIZE + 1\n",
        "\n",
        "lr_schedule = lambda t: np.interp([t], [0, (EPOCHS+1)//5, EPOCHS], [0, LEARNING_RATE, 0])[0]\n",
        "global_step = tf.train.get_or_create_global_step()\n",
        "lr_func = lambda: lr_schedule(global_step/batches_per_epoch)/BATCH_SIZE\n",
        "opt = tf.train.MomentumOptimizer(lr_func, momentum=MOMENTUM, use_nesterov=True)\n",
        "data_aug = lambda x, y: (tf.image.random_flip_left_right(tf.random_crop(x, [32, 32, 3])), y)\n",
        "\n",
        "\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
        "             loss='categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eeqp9zgtQjQN",
        "colab_type": "code",
        "outputId": "d7562c4e-6af4-4858-d4a2-b08544e2f5ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "t = time.time()\n",
        "test_set = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  train_loss = test_loss = train_acc = test_acc = 0.0\n",
        "  train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(data_aug).shuffle(len_train).batch(BATCH_SIZE).prefetch(1)\n",
        "  //train_set = tfe.add_execution_callback(LR_Finder(start_lr=1e-5, end_lr=10, step_size=np.ceil(x_train.shape[0]/BATCH_SIZE)))\n",
        "  tf.keras.backend.set_learning_phase(1)\n",
        "  for (x, y) in tqdm(train_set):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss, correct = model(x, y)\n",
        "\n",
        "    var = model.trainable_variables\n",
        "    grads = tape.gradient(loss, var)\n",
        "    for g, v in zip(grads, var):\n",
        "      g += v * WEIGHT_DECAY * BATCH_SIZE\n",
        "    opt.apply_gradients(zip(grads, var), global_step=global_step)\n",
        "\n",
        "    train_loss += loss.numpy()\n",
        "    train_acc += correct.numpy()\n",
        "\n",
        "  tf.keras.backend.set_learning_phase(0)\n",
        "  for (x, y) in test_set:\n",
        "    loss, correct = model(x, y)\n",
        "    test_loss += loss.numpy()\n",
        "    test_acc += correct.numpy()\n",
        "    \n",
        "  print('epoch:', epoch+1, 'lr:', lr_schedule(epoch+1), 'train loss:', train_loss / len_train, 'train acc:', train_acc / len_train, 'val loss:', test_loss / len_test, 'val acc:', test_acc / len_test, 'time:', time.time() - t)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-36d7f5e93212>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    (/train_set, =, tfe.add_execution_callback(LR_Finder(start_lr=1e-5,, end_lr=10,, step_size=np.ceil(x_train.shape[0]/BATCH_SIZE))))\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}