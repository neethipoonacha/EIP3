{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eip3.session2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neethipoonacha/EIP3/blob/master/eip3_session2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0QAJ5Odd0Cd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "576d83db-0a47-4f36-efed-df27b77fe99e"
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqZKW9UEfApE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "3da8e6cd-4cd9-4a3c-afb0-11981819a216"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xbqz-t8eVtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"/content/gdrive/My Drive/wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nWQghwhGI6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrxmNRuJG4Pa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6a4d0f74-e7b8-41ca-d2c2-d6ab11dd5c77"
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  163344\n",
            "Total Vocab:  58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGmNjnu8HAar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aed3d4ab-4c95-4db9-ce4e-9245c0b7272f"
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  163244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_5-SwV0HMIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVkBWMnEHR9k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "ba1308c4-f547-4b72-e0db-8f05cfbb2955"
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0726 04:43:53.977230 140425579214720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0726 04:43:54.013309 140425579214720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0726 04:43:54.021646 140425579214720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0726 04:43:54.377609 140425579214720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0726 04:43:54.388933 140425579214720 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0726 04:43:54.419111 140425579214720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0726 04:43:54.446444 140425579214720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVAh5cedJtpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8KdHiCMJx4J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a3d83d6-016a-46c8-9932-943f1c83635c"
      },
      "source": [
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0726 04:44:01.952933 140425579214720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "163244/163244 [==============================] - 1058s 6ms/step - loss: 2.9877\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.98770, saving model to weights-improvement-01-2.9877.hdf5\n",
            "Epoch 2/20\n",
            "163244/163244 [==============================] - 1059s 6ms/step - loss: 2.8107\n",
            "\n",
            "Epoch 00002: loss improved from 2.98770 to 2.81075, saving model to weights-improvement-02-2.8107.hdf5\n",
            "Epoch 3/20\n",
            "163244/163244 [==============================] - 1058s 6ms/step - loss: 2.7341\n",
            "\n",
            "Epoch 00003: loss improved from 2.81075 to 2.73414, saving model to weights-improvement-03-2.7341.hdf5\n",
            "Epoch 4/20\n",
            "163244/163244 [==============================] - 1054s 6ms/step - loss: 2.6750\n",
            "\n",
            "Epoch 00004: loss improved from 2.73414 to 2.67495, saving model to weights-improvement-04-2.6750.hdf5\n",
            "Epoch 5/20\n",
            "163244/163244 [==============================] - 1049s 6ms/step - loss: 2.6127\n",
            "\n",
            "Epoch 00005: loss improved from 2.67495 to 2.61266, saving model to weights-improvement-05-2.6127.hdf5\n",
            "Epoch 6/20\n",
            "163244/163244 [==============================] - 1054s 6ms/step - loss: 2.5568\n",
            "\n",
            "Epoch 00006: loss improved from 2.61266 to 2.55679, saving model to weights-improvement-06-2.5568.hdf5\n",
            "Epoch 7/20\n",
            "163244/163244 [==============================] - 1056s 6ms/step - loss: 2.5008\n",
            "\n",
            "Epoch 00007: loss improved from 2.55679 to 2.50080, saving model to weights-improvement-07-2.5008.hdf5\n",
            "Epoch 8/20\n",
            "163244/163244 [==============================] - 1050s 6ms/step - loss: 2.4542\n",
            "\n",
            "Epoch 00008: loss improved from 2.50080 to 2.45418, saving model to weights-improvement-08-2.4542.hdf5\n",
            "Epoch 9/20\n",
            "163244/163244 [==============================] - 1058s 6ms/step - loss: 2.4100\n",
            "\n",
            "Epoch 00009: loss improved from 2.45418 to 2.41004, saving model to weights-improvement-09-2.4100.hdf5\n",
            "Epoch 10/20\n",
            "163244/163244 [==============================] - 1050s 6ms/step - loss: 2.3691\n",
            "\n",
            "Epoch 00010: loss improved from 2.41004 to 2.36910, saving model to weights-improvement-10-2.3691.hdf5\n",
            "Epoch 11/20\n",
            "163244/163244 [==============================] - 1046s 6ms/step - loss: 2.3298\n",
            "\n",
            "Epoch 00011: loss improved from 2.36910 to 2.32980, saving model to weights-improvement-11-2.3298.hdf5\n",
            "Epoch 12/20\n",
            "163244/163244 [==============================] - 1055s 6ms/step - loss: 2.2942\n",
            "\n",
            "Epoch 00012: loss improved from 2.32980 to 2.29422, saving model to weights-improvement-12-2.2942.hdf5\n",
            "Epoch 13/20\n",
            "163244/163244 [==============================] - 1050s 6ms/step - loss: 2.2616\n",
            "\n",
            "Epoch 00013: loss improved from 2.29422 to 2.26157, saving model to weights-improvement-13-2.2616.hdf5\n",
            "Epoch 14/20\n",
            "163244/163244 [==============================] - 1053s 6ms/step - loss: 2.2270\n",
            "\n",
            "Epoch 00014: loss improved from 2.26157 to 2.22697, saving model to weights-improvement-14-2.2270.hdf5\n",
            "Epoch 15/20\n",
            "163244/163244 [==============================] - 1053s 6ms/step - loss: 2.1962\n",
            "\n",
            "Epoch 00015: loss improved from 2.22697 to 2.19624, saving model to weights-improvement-15-2.1962.hdf5\n",
            "Epoch 16/20\n",
            "163244/163244 [==============================] - 1054s 6ms/step - loss: 2.1659\n",
            "\n",
            "Epoch 00016: loss improved from 2.19624 to 2.16595, saving model to weights-improvement-16-2.1659.hdf5\n",
            "Epoch 17/20\n",
            "163244/163244 [==============================] - 1059s 6ms/step - loss: 2.1380\n",
            "\n",
            "Epoch 00017: loss improved from 2.16595 to 2.13803, saving model to weights-improvement-17-2.1380.hdf5\n",
            "Epoch 18/20\n",
            "163244/163244 [==============================] - 1054s 6ms/step - loss: 2.1129\n",
            "\n",
            "Epoch 00018: loss improved from 2.13803 to 2.11293, saving model to weights-improvement-18-2.1129.hdf5\n",
            "Epoch 19/20\n",
            "163244/163244 [==============================] - 1056s 6ms/step - loss: 2.0902\n",
            "\n",
            "Epoch 00019: loss improved from 2.11293 to 2.09016, saving model to weights-improvement-19-2.0902.hdf5\n",
            "Epoch 20/20\n",
            "163244/163244 [==============================] - 1055s 6ms/step - loss: 2.0634\n",
            "\n",
            "Epoch 00020: loss improved from 2.09016 to 2.06336, saving model to weights-improvement-20-2.0634.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb73da7d9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRT1RgMq96v9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = drive.CreateFile()\n",
        "f.SetContentFile(\"weights-improvement-19-2.0902.hdf5\")\n",
        "f.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR7PzRwD3PFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the network weights\n",
        "filename = \"weights-improvement-19-2.0902.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moUdlhvw3eYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRiOjqvm3flL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "a58d7d1d-3faa-4472-9168-1b91d0f9fc61"
      },
      "source": [
        "# pick a random seed\n",
        "import sys\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" result seemed to follow,\n",
            "except a little shaking among the distant green leaves.\n",
            "\n",
            "as there seemed to \"\n",
            " the whrt woen i soone  she would bete to be a garter or the tooed an it was to the whrt of the woodd \n",
            "\n",
            "ani the hert tas toin to the whrt wae the tas oo the war of the woide. \n",
            "'leee th the the garter an i cad'' said the march hare.\n",
            "\n",
            "'ie tou doe't kn whth the soiet ' said the march hare.\n",
            "\n",
            "'ie tou doe't kn whth the soiet ' said the march hare.\n",
            "\n",
            "'ie tou doe't kn whth the soiet ' said the march hare.\n",
            "\n",
            "'ie tou doe't kn whth the soiet ' said the march hare.\n",
            "\n",
            "'ie tou doe't kn whth the soiet ' said the march hare.\n",
            "\n",
            "'ie tou doe't kn whth the soiet ' said the march hare.\n",
            "\n",
            "'ie tou doe't kn whth the soiet ' said the march hare.\n",
            "\n",
            "'ie tou doe't kn whth the soiet ' said the march hare.\n",
            "\n",
            "'ie tou doe't kn whth the soiet ' said the march hare.\n",
            "\n",
            "'ie tou doe't kn whth the soiet ' said the march hare.\n",
            "\n",
            "'ie tou doe't kn whth the soiet ' said the march hare.\n",
            "\n",
            "'ie tou doe't kn whth the soiet ' said the march hare.\n",
            "\n",
            "'ie tou doe't kn whth the soiet ' said the march hare.\n",
            "\n",
            "'ie tou doe't kn whth the soiet ' said \n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH81MaSu5TdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c74e257-8d49-4484-95f2-f5f3e5be8bed"
      },
      "source": [
        "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"/content/gdrive/My Drive/wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()\n",
        "#add padding to the text\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# pad sequence\n",
        "\n",
        "#padded = pad_sequences(raw_text, maxlen=5)\n",
        "#print(padded)\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "#maxlen = max(dataY)\n",
        "\n",
        "#print (\"Max length of character is \",dataX,\":\",max(dataX))\n",
        "print (\"Max length of character Y \",dataY,\":\",max(dataY))\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "padded = pad_sequences(y, maxlen=max(dataY))\n",
        "print(\"padded\",padded)\n",
        "print(\"y is \",y)\n",
        "print(\"X is \",X)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(padded.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"/content/gdrive/My Drive/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "#f = drive.CreateFile()\n",
        "#f.SetContentFile(filepath)\n",
        "#f.Upload()\n",
        "# fit the model\n",
        "model.fit(X, padded, epochs=100, batch_size=64, callbacks=callbacks_list)\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  510\n",
            "Total Vocab:  31\n",
            "Max length of character Y  [1, 12, 22, 7, 1, 22, 21, 11, 13, 1, 22, 24, 1, 26, 29, 17, 11, 13, 1, 25, 16, 13, 1, 16, 9, 12, 1, 23, 13, 13, 23, 13, 12, 1, 17, 21, 26, 22, 1, 26, 16, 13, 0, 10, 22, 22, 18, 1, 16, 13, 24, 1, 25, 17, 25, 26, 13, 24, 1, 29, 9, 25, 1, 24, 13, 9, 12, 17, 21, 15, 5, 1, 10, 27, 26, 1, 17, 26, 1, 16, 9, 12, 1, 21, 22, 1, 23, 17, 11, 26, 27, 24, 13, 25, 1, 22, 24, 1, 11, 22, 21, 28, 13, 24, 25, 9, 26, 17, 22, 21, 25, 1, 17, 21, 0, 17, 26, 5, 1, 2, 9, 21, 12, 1, 29, 16, 9, 26, 1, 17, 25, 1, 26, 16, 13, 1, 27, 25, 13, 1, 22, 14, 1, 9, 1, 10, 22, 22, 18, 5, 2, 1, 26, 16, 22, 27, 15, 16, 26, 1, 9, 19, 17, 11, 13, 1, 2, 29, 17, 26, 16, 22, 27, 26, 1, 23, 17, 11, 26, 27, 24, 13, 25, 1, 22, 24, 0, 11, 22, 21, 28, 13, 24, 25, 9, 26, 17, 22, 21, 25, 8, 2, 0, 25, 22, 1, 25, 16, 13, 1, 29, 9, 25, 1, 11, 22, 21, 25, 17, 12, 13, 24, 17, 21, 15, 1, 17, 21, 1, 16, 13, 24, 1, 22, 29, 21, 1, 20, 17, 21, 12, 1, 3, 9, 25, 1, 29, 13, 19, 19, 1, 9, 25, 1, 25, 16, 13, 1, 11, 22, 27, 19, 12, 5, 1, 14, 22, 24, 1, 26, 16, 13, 0, 16, 22, 26, 1, 12, 9, 30, 1, 20, 9, 12, 13, 1, 16, 13, 24, 1, 14, 13, 13, 19, 1, 28, 13, 24, 30, 1, 25, 19, 13, 13, 23, 30, 1, 9, 21, 12, 1, 25, 26, 27, 23, 17, 12, 4, 5, 1, 29, 16, 13, 26, 16, 13, 24, 1, 26, 16, 13, 1, 23, 19, 13, 9, 25, 27, 24, 13, 0, 22, 14, 1, 20, 9, 18, 17, 21, 15, 1, 9, 1, 12, 9, 17, 25, 30, 6, 11, 16, 9, 17, 21, 1, 29, 22, 27, 19, 12, 1, 10, 13, 1, 29, 22, 24, 26, 16, 1, 26, 16, 13, 1, 26, 24, 22, 27, 10, 19, 13, 1, 22, 14, 1, 15, 13, 26, 26, 17, 21, 15, 1, 27, 23, 1, 0, 0, 0, 0] : 30\n",
            "Total Patterns:  410\n",
            "padded [[1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "y is  [[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n",
            "X is  [[[0.29032258]\n",
            "  [0.61290323]\n",
            "  [0.5483871 ]\n",
            "  ...\n",
            "  [0.03225806]\n",
            "  [0.83870968]\n",
            "  [0.70967742]]\n",
            "\n",
            " [[0.61290323]\n",
            "  [0.5483871 ]\n",
            "  [0.35483871]\n",
            "  ...\n",
            "  [0.83870968]\n",
            "  [0.70967742]\n",
            "  [0.03225806]]\n",
            "\n",
            " [[0.5483871 ]\n",
            "  [0.35483871]\n",
            "  [0.41935484]\n",
            "  ...\n",
            "  [0.70967742]\n",
            "  [0.03225806]\n",
            "  [0.38709677]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.29032258]\n",
            "  [0.67741935]\n",
            "  [0.38709677]\n",
            "  ...\n",
            "  [0.74193548]\n",
            "  [0.03225806]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.67741935]\n",
            "  [0.38709677]\n",
            "  [0.03225806]\n",
            "  ...\n",
            "  [0.03225806]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.38709677]\n",
            "  [0.03225806]\n",
            "  [0.80645161]\n",
            "  ...\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]]\n",
            "Epoch 1/100\n",
            "410/410 [==============================] - 14s 33ms/step - loss: 3.1829\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.18286, saving model to /content/gdrive/My Drive/weights-improvement-01-3.1829-bigger.hdf5\n",
            "Epoch 2/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.9259\n",
            "\n",
            "Epoch 00002: loss improved from 3.18286 to 2.92592, saving model to /content/gdrive/My Drive/weights-improvement-02-2.9259-bigger.hdf5\n",
            "Epoch 3/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.8869\n",
            "\n",
            "Epoch 00003: loss improved from 2.92592 to 2.88687, saving model to /content/gdrive/My Drive/weights-improvement-03-2.8869-bigger.hdf5\n",
            "Epoch 4/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.8693\n",
            "\n",
            "Epoch 00004: loss improved from 2.88687 to 2.86926, saving model to /content/gdrive/My Drive/weights-improvement-04-2.8693-bigger.hdf5\n",
            "Epoch 5/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8658\n",
            "\n",
            "Epoch 00005: loss improved from 2.86926 to 2.86578, saving model to /content/gdrive/My Drive/weights-improvement-05-2.8658-bigger.hdf5\n",
            "Epoch 6/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8588\n",
            "\n",
            "Epoch 00006: loss improved from 2.86578 to 2.85885, saving model to /content/gdrive/My Drive/weights-improvement-06-2.8588-bigger.hdf5\n",
            "Epoch 7/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8616\n",
            "\n",
            "Epoch 00007: loss did not improve from 2.85885\n",
            "Epoch 8/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8666\n",
            "\n",
            "Epoch 00008: loss did not improve from 2.85885\n",
            "Epoch 9/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8702\n",
            "\n",
            "Epoch 00009: loss did not improve from 2.85885\n",
            "Epoch 10/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8617\n",
            "\n",
            "Epoch 00010: loss did not improve from 2.85885\n",
            "Epoch 11/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8590\n",
            "\n",
            "Epoch 00011: loss did not improve from 2.85885\n",
            "Epoch 12/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8585\n",
            "\n",
            "Epoch 00012: loss improved from 2.85885 to 2.85851, saving model to /content/gdrive/My Drive/weights-improvement-12-2.8585-bigger.hdf5\n",
            "Epoch 13/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8592\n",
            "\n",
            "Epoch 00013: loss did not improve from 2.85851\n",
            "Epoch 14/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8529\n",
            "\n",
            "Epoch 00014: loss improved from 2.85851 to 2.85285, saving model to /content/gdrive/My Drive/weights-improvement-14-2.8529-bigger.hdf5\n",
            "Epoch 15/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8549\n",
            "\n",
            "Epoch 00015: loss did not improve from 2.85285\n",
            "Epoch 16/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8510\n",
            "\n",
            "Epoch 00016: loss improved from 2.85285 to 2.85097, saving model to /content/gdrive/My Drive/weights-improvement-16-2.8510-bigger.hdf5\n",
            "Epoch 17/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8578\n",
            "\n",
            "Epoch 00017: loss did not improve from 2.85097\n",
            "Epoch 18/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8521\n",
            "\n",
            "Epoch 00018: loss did not improve from 2.85097\n",
            "Epoch 19/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8610\n",
            "\n",
            "Epoch 00019: loss did not improve from 2.85097\n",
            "Epoch 20/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8563\n",
            "\n",
            "Epoch 00020: loss did not improve from 2.85097\n",
            "Epoch 21/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8547\n",
            "\n",
            "Epoch 00021: loss did not improve from 2.85097\n",
            "Epoch 22/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8484\n",
            "\n",
            "Epoch 00022: loss improved from 2.85097 to 2.84841, saving model to /content/gdrive/My Drive/weights-improvement-22-2.8484-bigger.hdf5\n",
            "Epoch 23/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8537\n",
            "\n",
            "Epoch 00023: loss did not improve from 2.84841\n",
            "Epoch 24/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8461\n",
            "\n",
            "Epoch 00024: loss improved from 2.84841 to 2.84609, saving model to /content/gdrive/My Drive/weights-improvement-24-2.8461-bigger.hdf5\n",
            "Epoch 25/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8458\n",
            "\n",
            "Epoch 00025: loss improved from 2.84609 to 2.84575, saving model to /content/gdrive/My Drive/weights-improvement-25-2.8458-bigger.hdf5\n",
            "Epoch 26/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8498\n",
            "\n",
            "Epoch 00026: loss did not improve from 2.84575\n",
            "Epoch 27/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8452\n",
            "\n",
            "Epoch 00027: loss improved from 2.84575 to 2.84521, saving model to /content/gdrive/My Drive/weights-improvement-27-2.8452-bigger.hdf5\n",
            "Epoch 28/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8389\n",
            "\n",
            "Epoch 00028: loss improved from 2.84521 to 2.83891, saving model to /content/gdrive/My Drive/weights-improvement-28-2.8389-bigger.hdf5\n",
            "Epoch 29/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8502\n",
            "\n",
            "Epoch 00029: loss did not improve from 2.83891\n",
            "Epoch 30/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8510\n",
            "\n",
            "Epoch 00030: loss did not improve from 2.83891\n",
            "Epoch 31/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8463\n",
            "\n",
            "Epoch 00031: loss did not improve from 2.83891\n",
            "Epoch 32/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8421\n",
            "\n",
            "Epoch 00032: loss did not improve from 2.83891\n",
            "Epoch 33/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8414\n",
            "\n",
            "Epoch 00033: loss did not improve from 2.83891\n",
            "Epoch 34/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8408\n",
            "\n",
            "Epoch 00034: loss did not improve from 2.83891\n",
            "Epoch 35/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.8379\n",
            "\n",
            "Epoch 00035: loss improved from 2.83891 to 2.83786, saving model to /content/gdrive/My Drive/weights-improvement-35-2.8379-bigger.hdf5\n",
            "Epoch 36/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8398\n",
            "\n",
            "Epoch 00036: loss did not improve from 2.83786\n",
            "Epoch 37/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8377\n",
            "\n",
            "Epoch 00037: loss improved from 2.83786 to 2.83772, saving model to /content/gdrive/My Drive/weights-improvement-37-2.8377-bigger.hdf5\n",
            "Epoch 38/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8325\n",
            "\n",
            "Epoch 00038: loss improved from 2.83772 to 2.83251, saving model to /content/gdrive/My Drive/weights-improvement-38-2.8325-bigger.hdf5\n",
            "Epoch 39/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8334\n",
            "\n",
            "Epoch 00039: loss did not improve from 2.83251\n",
            "Epoch 40/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8222\n",
            "\n",
            "Epoch 00040: loss improved from 2.83251 to 2.82225, saving model to /content/gdrive/My Drive/weights-improvement-40-2.8222-bigger.hdf5\n",
            "Epoch 41/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.8210\n",
            "\n",
            "Epoch 00041: loss improved from 2.82225 to 2.82101, saving model to /content/gdrive/My Drive/weights-improvement-41-2.8210-bigger.hdf5\n",
            "Epoch 42/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.8100\n",
            "\n",
            "Epoch 00042: loss improved from 2.82101 to 2.81001, saving model to /content/gdrive/My Drive/weights-improvement-42-2.8100-bigger.hdf5\n",
            "Epoch 43/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.7979\n",
            "\n",
            "Epoch 00043: loss improved from 2.81001 to 2.79792, saving model to /content/gdrive/My Drive/weights-improvement-43-2.7979-bigger.hdf5\n",
            "Epoch 44/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.7887\n",
            "\n",
            "Epoch 00044: loss improved from 2.79792 to 2.78870, saving model to /content/gdrive/My Drive/weights-improvement-44-2.7887-bigger.hdf5\n",
            "Epoch 45/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.7801\n",
            "\n",
            "Epoch 00045: loss improved from 2.78870 to 2.78009, saving model to /content/gdrive/My Drive/weights-improvement-45-2.7801-bigger.hdf5\n",
            "Epoch 46/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.7639\n",
            "\n",
            "Epoch 00046: loss improved from 2.78009 to 2.76393, saving model to /content/gdrive/My Drive/weights-improvement-46-2.7639-bigger.hdf5\n",
            "Epoch 47/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.8028\n",
            "\n",
            "Epoch 00047: loss did not improve from 2.76393\n",
            "Epoch 48/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.7761\n",
            "\n",
            "Epoch 00048: loss did not improve from 2.76393\n",
            "Epoch 49/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.7615\n",
            "\n",
            "Epoch 00049: loss improved from 2.76393 to 2.76152, saving model to /content/gdrive/My Drive/weights-improvement-49-2.7615-bigger.hdf5\n",
            "Epoch 50/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.7461\n",
            "\n",
            "Epoch 00050: loss improved from 2.76152 to 2.74608, saving model to /content/gdrive/My Drive/weights-improvement-50-2.7461-bigger.hdf5\n",
            "Epoch 51/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.7552\n",
            "\n",
            "Epoch 00051: loss did not improve from 2.74608\n",
            "Epoch 52/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.7313\n",
            "\n",
            "Epoch 00052: loss improved from 2.74608 to 2.73126, saving model to /content/gdrive/My Drive/weights-improvement-52-2.7313-bigger.hdf5\n",
            "Epoch 53/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.7408\n",
            "\n",
            "Epoch 00053: loss did not improve from 2.73126\n",
            "Epoch 54/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.7128\n",
            "\n",
            "Epoch 00054: loss improved from 2.73126 to 2.71283, saving model to /content/gdrive/My Drive/weights-improvement-54-2.7128-bigger.hdf5\n",
            "Epoch 55/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.6844\n",
            "\n",
            "Epoch 00055: loss improved from 2.71283 to 2.68440, saving model to /content/gdrive/My Drive/weights-improvement-55-2.6844-bigger.hdf5\n",
            "Epoch 56/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.6537\n",
            "\n",
            "Epoch 00056: loss improved from 2.68440 to 2.65370, saving model to /content/gdrive/My Drive/weights-improvement-56-2.6537-bigger.hdf5\n",
            "Epoch 57/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.6342\n",
            "\n",
            "Epoch 00057: loss improved from 2.65370 to 2.63415, saving model to /content/gdrive/My Drive/weights-improvement-57-2.6342-bigger.hdf5\n",
            "Epoch 58/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.6142\n",
            "\n",
            "Epoch 00058: loss improved from 2.63415 to 2.61416, saving model to /content/gdrive/My Drive/weights-improvement-58-2.6142-bigger.hdf5\n",
            "Epoch 59/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.5885\n",
            "\n",
            "Epoch 00059: loss improved from 2.61416 to 2.58847, saving model to /content/gdrive/My Drive/weights-improvement-59-2.5885-bigger.hdf5\n",
            "Epoch 60/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.5628\n",
            "\n",
            "Epoch 00060: loss improved from 2.58847 to 2.56284, saving model to /content/gdrive/My Drive/weights-improvement-60-2.5628-bigger.hdf5\n",
            "Epoch 61/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.5357\n",
            "\n",
            "Epoch 00061: loss improved from 2.56284 to 2.53568, saving model to /content/gdrive/My Drive/weights-improvement-61-2.5357-bigger.hdf5\n",
            "Epoch 62/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.5025\n",
            "\n",
            "Epoch 00062: loss improved from 2.53568 to 2.50250, saving model to /content/gdrive/My Drive/weights-improvement-62-2.5025-bigger.hdf5\n",
            "Epoch 63/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.4913\n",
            "\n",
            "Epoch 00063: loss improved from 2.50250 to 2.49133, saving model to /content/gdrive/My Drive/weights-improvement-63-2.4913-bigger.hdf5\n",
            "Epoch 64/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.4576\n",
            "\n",
            "Epoch 00064: loss improved from 2.49133 to 2.45757, saving model to /content/gdrive/My Drive/weights-improvement-64-2.4576-bigger.hdf5\n",
            "Epoch 65/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.3947\n",
            "\n",
            "Epoch 00065: loss improved from 2.45757 to 2.39470, saving model to /content/gdrive/My Drive/weights-improvement-65-2.3947-bigger.hdf5\n",
            "Epoch 66/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.3454\n",
            "\n",
            "Epoch 00066: loss improved from 2.39470 to 2.34535, saving model to /content/gdrive/My Drive/weights-improvement-66-2.3454-bigger.hdf5\n",
            "Epoch 67/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.3176\n",
            "\n",
            "Epoch 00067: loss improved from 2.34535 to 2.31757, saving model to /content/gdrive/My Drive/weights-improvement-67-2.3176-bigger.hdf5\n",
            "Epoch 68/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 2.2676\n",
            "\n",
            "Epoch 00068: loss improved from 2.31757 to 2.26758, saving model to /content/gdrive/My Drive/weights-improvement-68-2.2676-bigger.hdf5\n",
            "Epoch 69/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.2328\n",
            "\n",
            "Epoch 00069: loss improved from 2.26758 to 2.23276, saving model to /content/gdrive/My Drive/weights-improvement-69-2.2328-bigger.hdf5\n",
            "Epoch 70/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.1650\n",
            "\n",
            "Epoch 00070: loss improved from 2.23276 to 2.16495, saving model to /content/gdrive/My Drive/weights-improvement-70-2.1650-bigger.hdf5\n",
            "Epoch 71/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 2.0708\n",
            "\n",
            "Epoch 00071: loss improved from 2.16495 to 2.07082, saving model to /content/gdrive/My Drive/weights-improvement-71-2.0708-bigger.hdf5\n",
            "Epoch 72/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 1.9524\n",
            "\n",
            "Epoch 00072: loss improved from 2.07082 to 1.95245, saving model to /content/gdrive/My Drive/weights-improvement-72-1.9524-bigger.hdf5\n",
            "Epoch 73/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 1.8714\n",
            "\n",
            "Epoch 00073: loss improved from 1.95245 to 1.87144, saving model to /content/gdrive/My Drive/weights-improvement-73-1.8714-bigger.hdf5\n",
            "Epoch 74/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 1.7955\n",
            "\n",
            "Epoch 00074: loss improved from 1.87144 to 1.79552, saving model to /content/gdrive/My Drive/weights-improvement-74-1.7955-bigger.hdf5\n",
            "Epoch 75/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 1.7295\n",
            "\n",
            "Epoch 00075: loss improved from 1.79552 to 1.72946, saving model to /content/gdrive/My Drive/weights-improvement-75-1.7295-bigger.hdf5\n",
            "Epoch 76/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 1.6293\n",
            "\n",
            "Epoch 00076: loss improved from 1.72946 to 1.62926, saving model to /content/gdrive/My Drive/weights-improvement-76-1.6293-bigger.hdf5\n",
            "Epoch 77/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 1.5430\n",
            "\n",
            "Epoch 00077: loss improved from 1.62926 to 1.54303, saving model to /content/gdrive/My Drive/weights-improvement-77-1.5430-bigger.hdf5\n",
            "Epoch 78/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 1.4485\n",
            "\n",
            "Epoch 00078: loss improved from 1.54303 to 1.44851, saving model to /content/gdrive/My Drive/weights-improvement-78-1.4485-bigger.hdf5\n",
            "Epoch 79/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 1.4660\n",
            "\n",
            "Epoch 00079: loss did not improve from 1.44851\n",
            "Epoch 80/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 1.3254\n",
            "\n",
            "Epoch 00080: loss improved from 1.44851 to 1.32541, saving model to /content/gdrive/My Drive/weights-improvement-80-1.3254-bigger.hdf5\n",
            "Epoch 81/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 1.2347\n",
            "\n",
            "Epoch 00081: loss improved from 1.32541 to 1.23470, saving model to /content/gdrive/My Drive/weights-improvement-81-1.2347-bigger.hdf5\n",
            "Epoch 82/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 1.1449\n",
            "\n",
            "Epoch 00082: loss improved from 1.23470 to 1.14486, saving model to /content/gdrive/My Drive/weights-improvement-82-1.1449-bigger.hdf5\n",
            "Epoch 83/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 1.0081\n",
            "\n",
            "Epoch 00083: loss improved from 1.14486 to 1.00809, saving model to /content/gdrive/My Drive/weights-improvement-83-1.0081-bigger.hdf5\n",
            "Epoch 84/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 0.9112\n",
            "\n",
            "Epoch 00084: loss improved from 1.00809 to 0.91117, saving model to /content/gdrive/My Drive/weights-improvement-84-0.9112-bigger.hdf5\n",
            "Epoch 85/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 0.8257\n",
            "\n",
            "Epoch 00085: loss improved from 0.91117 to 0.82568, saving model to /content/gdrive/My Drive/weights-improvement-85-0.8257-bigger.hdf5\n",
            "Epoch 86/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 0.7528\n",
            "\n",
            "Epoch 00086: loss improved from 0.82568 to 0.75283, saving model to /content/gdrive/My Drive/weights-improvement-86-0.7528-bigger.hdf5\n",
            "Epoch 87/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 0.6420\n",
            "\n",
            "Epoch 00087: loss improved from 0.75283 to 0.64197, saving model to /content/gdrive/My Drive/weights-improvement-87-0.6420-bigger.hdf5\n",
            "Epoch 88/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 0.5751\n",
            "\n",
            "Epoch 00088: loss improved from 0.64197 to 0.57510, saving model to /content/gdrive/My Drive/weights-improvement-88-0.5751-bigger.hdf5\n",
            "Epoch 89/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 0.5143\n",
            "\n",
            "Epoch 00089: loss improved from 0.57510 to 0.51431, saving model to /content/gdrive/My Drive/weights-improvement-89-0.5143-bigger.hdf5\n",
            "Epoch 90/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 0.4620\n",
            "\n",
            "Epoch 00090: loss improved from 0.51431 to 0.46199, saving model to /content/gdrive/My Drive/weights-improvement-90-0.4620-bigger.hdf5\n",
            "Epoch 91/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 0.4072\n",
            "\n",
            "Epoch 00091: loss improved from 0.46199 to 0.40716, saving model to /content/gdrive/My Drive/weights-improvement-91-0.4072-bigger.hdf5\n",
            "Epoch 92/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 0.3680\n",
            "\n",
            "Epoch 00092: loss improved from 0.40716 to 0.36796, saving model to /content/gdrive/My Drive/weights-improvement-92-0.3680-bigger.hdf5\n",
            "Epoch 93/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 0.3294\n",
            "\n",
            "Epoch 00093: loss improved from 0.36796 to 0.32941, saving model to /content/gdrive/My Drive/weights-improvement-93-0.3294-bigger.hdf5\n",
            "Epoch 94/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 0.3084\n",
            "\n",
            "Epoch 00094: loss improved from 0.32941 to 0.30842, saving model to /content/gdrive/My Drive/weights-improvement-94-0.3084-bigger.hdf5\n",
            "Epoch 95/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 0.2697\n",
            "\n",
            "Epoch 00095: loss improved from 0.30842 to 0.26966, saving model to /content/gdrive/My Drive/weights-improvement-95-0.2697-bigger.hdf5\n",
            "Epoch 96/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 0.2464\n",
            "\n",
            "Epoch 00096: loss improved from 0.26966 to 0.24641, saving model to /content/gdrive/My Drive/weights-improvement-96-0.2464-bigger.hdf5\n",
            "Epoch 97/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 0.2151\n",
            "\n",
            "Epoch 00097: loss improved from 0.24641 to 0.21514, saving model to /content/gdrive/My Drive/weights-improvement-97-0.2151-bigger.hdf5\n",
            "Epoch 98/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 0.1963\n",
            "\n",
            "Epoch 00098: loss improved from 0.21514 to 0.19626, saving model to /content/gdrive/My Drive/weights-improvement-98-0.1963-bigger.hdf5\n",
            "Epoch 99/100\n",
            "410/410 [==============================] - 10s 24ms/step - loss: 0.1809\n",
            "\n",
            "Epoch 00099: loss improved from 0.19626 to 0.18089, saving model to /content/gdrive/My Drive/weights-improvement-99-0.1809-bigger.hdf5\n",
            "Epoch 100/100\n",
            "410/410 [==============================] - 10s 23ms/step - loss: 0.1640\n",
            "\n",
            "Epoch 00100: loss improved from 0.18089 to 0.16402, saving model to /content/gdrive/My Drive/weights-improvement-100-0.1640-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8176542860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3kvRoRW4zJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a50c94c8-3d27-4d1c-c0bd-49cb1f3bc6b8"
      },
      "source": [
        "# Load Larger LSTM network and generate text\n",
        "import sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"/content/gdrive/My Drive/wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()\n",
        "# create mapping of unique chars to integers, and a reverse mapping\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "padded = pad_sequences(y, maxlen=max(dataY))\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(padded.shape[1], activation='softmax'))\n",
        "# load the network weights\n",
        "filename = \"/content/gdrive/My Drive/weights-improvement-100-0.1640-bigger.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "#padded = pad_sequences(pattern, maxlen=max(dataY))\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  510\n",
            "Total Vocab:  31\n",
            "Total Patterns:  410\n",
            "Seed:\n",
            "\" he\n",
            "book her sister was reading, but it had no pictures or conversations in\n",
            "it, 'and what is the use  \"\n",
            "ne\n",
            "?\n",
            "anni) \n",
            "sgntfgs\n",
            "?khbd\n",
            " vhsgnts\n",
            "ohbstpdr\n",
            "np\n",
            "bnmudpr?shnmr: rm\n",
            "hdm\n",
            "hmmf\n",
            "nmm\n",
            "v??\n",
            "rm\n",
            "vdcc\n",
            "rg\n",
            "nndb??r\n",
            "sn\n",
            "occp\n",
            "sn\n",
            "\n",
            "?m\n",
            "sssk\n",
            "\n",
            "?r\n",
            "rrw,bg?h\n",
            "vnt\n",
            "c\n",
            "annpsggsgddssd\n",
            "nt\n",
            "cnp\n",
            "\n",
            "np\n",
            "sdddd\n",
            "rgn\n",
            "ohbs\n",
            "rp\n",
            "sn\n",
            "cm\n",
            "\n",
            "sst) \n",
            "sn\n",
            "vggs\n",
            "?hhbd\n",
            " vhsgnts\n",
            "ohbstpdr\n",
            "np\n",
            "bnmudpr?shnmr: hm\n",
            "rdd ??mc\n",
            "vgbshhhrmr: rm\n",
            "gd?\n",
            "nm\n",
            "\n",
            "nm\n",
            "svhb\n",
            ")\n",
            "vgdsgdp\n",
            "sgd\n",
            "okd?rtpd\n",
            "ne\n",
            "l?ihmf\n",
            "?\n",
            "c?hrw,bg?hm\n",
            "vntkc\n",
            "ad\n",
            "vnpsg\n",
            "sgd\n",
            "spntakd\n",
            "ne\n",
            "fdsshmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldishmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldsshmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "fdsshmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldishmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldsshm\n",
            "?\n",
            "c?hrw,bg?hm\n",
            "vntkc\n",
            "ad\n",
            "vnpsg\n",
            "sgd\n",
            "spntakd\n",
            "ne\n",
            "fdsshmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldishmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldsshmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "fdsshmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldishmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldsshm\n",
            "?\n",
            "c?hrw,bg?hm\n",
            "vntkc\n",
            "ad\n",
            "vnpsg\n",
            "sgd\n",
            "spntakd\n",
            "ne\n",
            "fdsshmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldishmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldsshmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "fdsshmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldishmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldsshm\n",
            "?\n",
            "c?hrw,bg?hm\n",
            "vntkc\n",
            "ad\n",
            "vnpsg\n",
            "sgd\n",
            "spntakd\n",
            "ne\n",
            "fdsshmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldishmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldsshmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "fdsshmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldishmf\n",
            "to\n",
            "sgdttkd\n",
            "ne\n",
            "ldsshm\n",
            "?\n",
            "c?hrw,bg?hm\n",
            "vntkc\n",
            "ad\n",
            "vnpsg\n",
            "sgd\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}